{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cervical cancer screening classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cervical cancer is easy to prevent if caught in its pre-cancerous stage. Today, women worldwide in low-resource settings are benefiting from programs where cancer is identified and treated in a single visit. However, due in part to lacking expertise in the field, one of the greatest challenges of these cervical cancer screen and treat programs is determining the appropriate method of treatment which can vary depending on patientsâ€™ physiological differences.\n",
    "\n",
    "The goal of this notebook is to train a model to correctly classify cervix types based on cervical images.\n",
    "\n",
    "These different types of cervix in the data set are all considered normal (not cancerous), but since the transformation zones aren't always visible, some of the patients require further testing while some don't. This decision is very important for the healthcare provider and critical for the patient. Identifying the transformation zones is not an easy task for the healthcare providers, therefore, an algorithm-aided decision will significantly improve the quality and efficiency of cervical cancer screening for these patients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "\n",
    "# Temporarily ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import our class, and instantiate\n",
    "import vgg16; reload(vgg16)\n",
    "from vgg16 import Vgg16\n",
    "\n",
    "vgg = Vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6731 images belonging to 3 classes.\n",
      "Found 1481 images belonging to 3 classes.\n",
      "Epoch 1/10\n",
      "6731/6731 [==============================] - 1329s - loss: 1.7163 - acc: 0.4597 - val_loss: 1.1468 - val_acc: 0.4700\n",
      "Epoch 2/10\n",
      "6731/6731 [==============================] - 1198s - loss: 1.4223 - acc: 0.4977 - val_loss: 1.0155 - val_acc: 0.5469\n",
      "Epoch 3/10\n",
      "6731/6731 [==============================] - 1225s - loss: 1.2723 - acc: 0.5025 - val_loss: 1.0428 - val_acc: 0.5544\n",
      "Epoch 4/10\n",
      "6731/6731 [==============================] - 1195s - loss: 1.1464 - acc: 0.5363 - val_loss: 1.0050 - val_acc: 0.5415\n",
      "Epoch 5/10\n",
      "6731/6731 [==============================] - 1217s - loss: 1.0988 - acc: 0.5466 - val_loss: 0.9931 - val_acc: 0.5753\n",
      "Epoch 6/10\n",
      "6731/6731 [==============================] - 1209s - loss: 1.0878 - acc: 0.5417 - val_loss: 1.0707 - val_acc: 0.5003\n",
      "Epoch 7/10\n",
      "6731/6731 [==============================] - 1229s - loss: 1.0830 - acc: 0.5491 - val_loss: 1.0583 - val_acc: 0.5591\n",
      "Epoch 8/10\n",
      "6731/6731 [==============================] - 1231s - loss: 1.0740 - acc: 0.5507 - val_loss: 1.0307 - val_acc: 0.5625\n",
      "Epoch 9/10\n",
      "6731/6731 [==============================] - 1217s - loss: 1.0647 - acc: 0.5479 - val_loss: 0.9611 - val_acc: 0.5665\n",
      "Epoch 10/10\n",
      "6731/6731 [==============================] - 1203s - loss: 1.0364 - acc: 0.5531 - val_loss: 1.0325 - val_acc: 0.5172\n"
     ]
    }
   ],
   "source": [
    "path = \"./data/\"\n",
    "batch_size=64\n",
    "\n",
    "# Grab a few images at a time for training and validation.\n",
    "# NB: They must be in subdirectories named based on their category\n",
    "batches = vgg.get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = vgg.get_batches(path+'valid', batch_size=batch_size*2)\n",
    "vgg.finetune(batches)\n",
    "vgg.fit(batches, val_batches, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
